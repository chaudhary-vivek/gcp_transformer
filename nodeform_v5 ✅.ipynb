{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Cora dataset with Noderformer. Trainformer is training with drop in loss. Complete with testing loop ✅ ✅ ✅ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import  remove_self_loops, add_self_loops\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from utils  import *\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import dropout_adj, to_undirected, to_networkx\n",
    "import time\n",
    "from ortools.sat.python import cp_model\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# NOTE: for consistent data splits, see data_utils.rand_train_test_idx\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "fix_seed(42)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to count violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_same_color_edges(graph, colors, num_classes):\n",
    "    count = 0\n",
    "    if colors != 'no_solution':\n",
    "        for u, v in graph.edges():\n",
    "            if colors[u] >= num_classes - 1:\n",
    "                count += 1\n",
    "            elif colors[v] >= num_classes - 1:\n",
    "                count += 1\n",
    "            elif colors[u] == colors[v]:\n",
    "                count += 1\n",
    "    else:\n",
    "        count = np.nan\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to make graph using ORTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_graph_with_ortools(G, chromatic_number):\n",
    "    # Create the model\n",
    "    model = cp_model.CpModel()\n",
    "\n",
    "    # Create variables\n",
    "    # For graph coloring, each node is a variable and the colors are their domains\n",
    "    color_vars = {\n",
    "        node: model.NewIntVar(0, chromatic_number - 1, f'node_{node}')\n",
    "        for node in G.nodes\n",
    "    }\n",
    "\n",
    "    # Create constraints\n",
    "    # Adjacent nodes must have different colors\n",
    "    for edge in G.edges:\n",
    "        model.Add(color_vars[edge[0]] != color_vars[edge[1]])\n",
    "\n",
    "    # Create a solver and solve the model\n",
    "    solver = cp_model.CpSolver()\n",
    "    status = solver.Solve(model)\n",
    "\n",
    "    if status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n",
    "        # If a solution exists, extract the colors\n",
    "        solution = {node: solver.Value(var) for node, var in color_vars.items()}\n",
    " \n",
    "        return solution\n",
    "    else:\n",
    "        # If the problem could not be solved, raise an error\n",
    "        solution =  'no_solution'\n",
    "        return solution\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to color Graphs using Greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_graph(G, k):\n",
    "    # Initialize the result dictionary with all vertices uncolored initially\n",
    "    coloring = {node: None for node in G.nodes()}\n",
    "\n",
    "    # Go through each node and assign the first available color\n",
    "    for node in G.nodes():\n",
    "        neighbor_colors = {coloring[neighbor] for neighbor in G.neighbors(node)}\n",
    "        available_colors = set(range(len(G))) - neighbor_colors\n",
    "\n",
    "        # Assign the first available color\n",
    "        for color in range(len(G)):\n",
    "            if color in available_colors:\n",
    "                coloring[node] = color\n",
    "                break\n",
    "\n",
    "    return coloring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to color graphs using DSATUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsatur(graph):\n",
    "    # Initialize color assignment and saturation degrees\n",
    "    colors = {node: None for node in graph.nodes()}\n",
    "    saturation_degrees = {node: 0 for node in graph.nodes()}\n",
    "\n",
    "    # Utility function to update saturation degrees\n",
    "    def update_saturation(node):\n",
    "        adjacent_colors = set(colors[neighbor] for neighbor in graph.neighbors(node) if colors[neighbor] is not None)\n",
    "        saturation_degrees[node] = len(adjacent_colors)\n",
    "\n",
    "    # Color assignment loop\n",
    "    while None in colors.values():\n",
    "        # Select the uncolored node with the highest saturation degree, breaking ties by degree\n",
    "        node_to_color = max(\n",
    "            (node for node in graph.nodes() if colors[node] is None),\n",
    "            key=lambda n: (saturation_degrees[n], len(list(graph.neighbors(n))))\n",
    "        )\n",
    "\n",
    "        # Find the first available color\n",
    "        used_colors = set(colors[neighbor] for neighbor in graph.neighbors(node_to_color) if colors[neighbor] is not None)\n",
    "        color = 0\n",
    "        while color in used_colors:\n",
    "            color += 1\n",
    "\n",
    "        colors[node_to_color] = color\n",
    "\n",
    "        # Update saturation degrees of adjacent nodes\n",
    "        for neighbor in graph.neighbors(node_to_color):\n",
    "            if colors[neighbor] is None:\n",
    "                update_saturation(neighbor)\n",
    "\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x_final):\n",
    "\n",
    "    max_args = torch.argmax(x_final, dim = 1)\n",
    "    gnn_sol = {index: value.item() for index, value in enumerate(max_args)}\n",
    "\n",
    "    return gnn_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(filepath):\n",
    "    with open(filepath,\"r\") as f:\n",
    "\n",
    "        line = ''\n",
    "\n",
    "        # Parse number of vertices\n",
    "        while 'DIMENSION' not in line: line = f.readline();\n",
    "        n = int(line.split()[1])\n",
    "        \n",
    "        Ma = np.zeros((n,n),dtype=int)\n",
    "        \n",
    "        # Parse edges\n",
    "        while 'EDGE_DATA_SECTION' not in line: line = f.readline();\n",
    "        line = f.readline()\n",
    "        while '-1' not in line:\n",
    "            i,j = [ int(x) for x in line.split() ]\n",
    "            Ma[i,j] = 1\n",
    "            line = f.readline()\n",
    "        #end while\n",
    "\n",
    "        # Parse diff edge\n",
    "        while 'DIFF_EDGE' not in line: line = f.readline();\n",
    "        diff_edge = [ int(x) for x in f.readline().split() ]\n",
    "\n",
    "        # Parse target cost\n",
    "        while 'CHROM_NUMBER' not in line: line = f.readline();\n",
    "        chrom_number = int(f.readline().strip())\n",
    "\n",
    "    #end\n",
    "    return Ma,chrom_number,diff_edge\n",
    "\n",
    "\n",
    "def get_data(num_classes, directory):\n",
    "    Mas = []\n",
    "    chrom_numbers = []\n",
    "    diff_edges = []\n",
    "    torch_dats = []\n",
    "    nx_graphs = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            Ma,chrom_number,diff_edge = read_graph(filepath)\n",
    "\n",
    "            adj_matrix = np.array(Ma)  # Replace with your n*n adjacency matrix\n",
    "\n",
    "            # Convert adjacency matrix to edge list\n",
    "            edge_index = np.array(adj_matrix.nonzero(), dtype=np.int64)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "            n = Ma.shape[0]\n",
    "            train_mask = torch.cat((torch.ones(n // 3, dtype=torch.bool), torch.zeros(2 * n // 3, dtype=torch.bool)))\n",
    "            val_mask = torch.cat((torch.zeros(n // 3, dtype=torch.bool), torch.ones(n // 3, dtype=torch.bool), torch.zeros(n // 3, dtype=torch.bool)))\n",
    "            test_mask = torch.cat((torch.zeros(2 * n // 3, dtype=torch.bool), torch.ones(n // 3, dtype=torch.bool)))\n",
    "\n",
    "\n",
    "            # Create dummy node features (e.g., one-hot encoding)\n",
    "            num_nodes = adj_matrix.shape[0]\n",
    "            node_features = torch.ones(num_nodes, num_classes)  \n",
    "\n",
    "            # Create PyTorch Geometric data object\n",
    "            data = Data(x=node_features, edge_index=edge_index)\n",
    "            data.y = torch.ones(n)\n",
    "            data.train_mask = train_mask\n",
    "            data.val_mask = val_mask\n",
    "            data.test_mask = test_mask\n",
    "            \n",
    "\n",
    "            # Creating networkx graphs\n",
    "            nx_graph = nx.from_numpy_array(adj_matrix)\n",
    "\n",
    "\n",
    "            Mas.append(Ma)\n",
    "            chrom_numbers.append(chrom_number)\n",
    "            diff_edges.append(diff_edge)\n",
    "            torch_dats.append(data)\n",
    "            nx_graphs.append(nx_graph)\n",
    "\n",
    "    chrom_list = []\n",
    "    for data,k, nx_graph in zip(torch_dats, chrom_numbers, nx_graphs):\n",
    "        chrom_list.append((data, k, nx_graph))\n",
    "\n",
    "    k_list = [item for item in chrom_list if item[1] == num_classes]\n",
    "\n",
    "    # Keep only the first 250 elements of the filtered list\n",
    "    k_list = k_list[:170]\n",
    "\n",
    "    \n",
    "    data = [item[0] for item in k_list]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCDataset(object):\n",
    "    def __init__(self, num_nodes, num_classes):\n",
    "\n",
    "        self.name = str(num_nodes) + '_' +str(num_classes)  # original name, e.g., ogbn-proteins\n",
    "        self.graph = {}\n",
    "        self.label = None\n",
    "\n",
    "    def get_idx_split(self,  train_prop=.5, valid_prop=.25):\n",
    "        \"\"\"\n",
    "        split_type: 'random' for random splitting, 'class' for splitting with equal node num per class\n",
    "        train_prop: The proportion of dataset for train split. Between 0 and 1.\n",
    "        valid_prop: The proportion of dataset for validation split. Between 0 and 1.\n",
    "        label_num_per_class: num of nodes per class\n",
    "        \"\"\"\n",
    "\n",
    "        ignore_negative = False if self.name == 'ogbn-proteins' else True\n",
    "        train_idx, valid_idx, test_idx = rand_train_test_idx(\n",
    "            self.label, train_prop=train_prop, valid_prop=valid_prop, ignore_negative=ignore_negative)\n",
    "        split_idx = {'train': train_idx,\n",
    "                        'valid': valid_idx,\n",
    "                        'test': test_idx}\n",
    "\n",
    "        return split_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx == 0, 'This dataset has only one graph'\n",
    "        return self.graph, self.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.__class__.__name__, len(self))\n",
    "    \n",
    "def load_dataset( num_nodes, num_classes, ind):\n",
    "\n",
    "    if num_nodes == 50:\n",
    "        directory = 'chromatic_graphs/n50'\n",
    "    elif num_nodes == 60:\n",
    "        directory = 'chromatic_graphs/n60'\n",
    "    elif num_nodes == 40:\n",
    "        directory = 'chromatic_graphs/n40'\n",
    "    elif num_nodes == 30:\n",
    "        directory = 'chromatic_graphs/n30'\n",
    "\n",
    "    data = get_data(num_classes, directory)\n",
    "    data = data[ind]\n",
    "\n",
    "\n",
    "    edge_index = data.edge_index\n",
    "    node_feat = data.x\n",
    "    label = data.y\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    dataset = NCDataset(num_nodes, num_classes)\n",
    "\n",
    "\n",
    "    dataset.graph = {'edge_index': edge_index,\n",
    "                     'node_feat': node_feat,\n",
    "                     'edge_feat': None,\n",
    "                     'num_nodes': num_nodes}\n",
    "    dataset.label = label\n",
    "\n",
    "    return dataset, data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, num_heads=4, dropout=0.9,\n",
    "                 kernel_transformation=softmax_kernel_transformation, nb_random_features=30, use_bn=True, use_gumbel=True,\n",
    "                 use_residual=True, use_act=False, use_jk=False, nb_gumbel_sample=10, rb_order=0, rb_trans='sigmoid', use_edge_loss=True):\n",
    "        super(NodeFormer, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.fcs = nn.ModuleList()\n",
    "        self.fcs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(\n",
    "                NodeFormerConv(hidden_channels, hidden_channels, num_heads=num_heads, kernel_transformation=kernel_transformation,\n",
    "                              nb_random_features=nb_random_features, use_gumbel=use_gumbel, nb_gumbel_sample=nb_gumbel_sample,\n",
    "                               rb_order=rb_order, rb_trans=rb_trans, use_edge_loss=use_edge_loss))\n",
    "            self.bns.append(nn.LayerNorm(hidden_channels))\n",
    "\n",
    "        if use_jk:\n",
    "            self.fcs.append(nn.Linear(hidden_channels * num_layers + hidden_channels, out_channels))\n",
    "        else:\n",
    "            self.fcs.append(nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.activation = F.relu\n",
    "        self.use_bn = use_bn\n",
    "        self.use_residual = use_residual\n",
    "        self.use_act = use_act\n",
    "        self.use_jk = use_jk\n",
    "        self.use_edge_loss = use_edge_loss\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, adjs, tau=1.0):\n",
    "\n",
    "        row, col = edge_index\n",
    "\n",
    "        x = x.unsqueeze(0) # [B, N, H, D], B=1 denotes number of graph\n",
    "        layer_ = []\n",
    "        link_loss_ = []\n",
    "  \n",
    "        z = self.fcs[0](x)     \n",
    "        if self.use_bn:\n",
    "            z = self.bns[0](z)\n",
    "        z = self.activation(z)\n",
    "        z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "\n",
    "        layer_.append(z)\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.use_edge_loss:\n",
    "                z, link_loss = conv(z, adjs, tau)\n",
    "                link_loss_.append(link_loss)\n",
    "            else:\n",
    "                z = conv(z, adjs, tau)\n",
    "            if self.use_residual:\n",
    "                z += layer_[i]\n",
    "            if self.use_bn:\n",
    "                z = self.bns[i+1](z)\n",
    "            if self.use_act:\n",
    "                z = self.activation(z)\n",
    "            z = F.dropout(z, p=self.dropout, training=self.training)\n",
    "            layer_.append(z)\n",
    "\n",
    "        if self.use_jk: # use jk connection for each layer\n",
    "            z = torch.cat(layer_, dim=-1)\n",
    "             \n",
    "        x_out = self.fcs[-1](z).squeeze(0)       \n",
    "        x_softmax = F.softmax(100*x_out, dim=1)\n",
    "\n",
    "        # Vectorized indexing\n",
    "        pi = x_softmax[col.long()]  # Assuming no_loop_col contains integer values\n",
    "        pj = x_softmax[row.long()]  # Assuming no_loop_row contains integer values  \n",
    "        prod = torch.mul(pi, pj)\n",
    "        loss = torch.sum(prod)\n",
    "\n",
    "        return x_out, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load method ###\n",
    "hidden_channels = 32\n",
    "num_layers = 2\n",
    "dropout = 0.9\n",
    "num_heads = 64\n",
    "use_bn = True\n",
    "M = 30\n",
    "use_gumbel = True\n",
    "use_residual = True\n",
    "use_act = True\n",
    "use_jk = True\n",
    "K = 10\n",
    "rb_order = 0\n",
    "rb_trans = 'sigmoid'\n",
    "\n",
    "lr = 0.005\n",
    "num_epochs = 350\n",
    "num_nodes = 60 # 30, 40, 50, 60\n",
    "num_classes = 4 # 4 or 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num nodes  30  num classes  4  index  0  gnn violation  55.56 cp violation  37.78 greedy violation  52.22 dsatur violation  44.44\n",
      "num nodes  30  num classes  4  index  1  gnn violation  64.0 cp violation  45.33 greedy violation  54.67 dsatur violation  46.67\n",
      "num nodes  30  num classes  4  index  2  gnn violation  59.38 cp violation  45.83 greedy violation  56.25 dsatur violation  59.38\n",
      "num nodes  30  num classes  4  index  3  gnn violation  68.48 cp violation  54.35 greedy violation  41.3 dsatur violation  53.26\n",
      "num nodes  30  num classes  4  index  4  gnn violation  61.18 cp violation  56.47 greedy violation  50.59 dsatur violation  47.06\n",
      "num nodes  30  num classes  4  index  5  gnn violation  57.83 cp violation  43.37 greedy violation  60.24 dsatur violation  32.53\n",
      "num nodes  30  num classes  4  index  6  gnn violation  65.17 cp violation  37.08 greedy violation  57.3 dsatur violation  46.07\n",
      "num nodes  30  num classes  4  index  7  gnn violation  64.52 cp violation  47.31 greedy violation  55.91 dsatur violation  54.84\n",
      "num nodes  30  num classes  4  index  8  gnn violation  39.13 cp violation  56.52 greedy violation  50.0 dsatur violation  46.74\n",
      "num nodes  30  num classes  4  index  9  gnn violation  50.98 cp violation  42.16 greedy violation  57.84 dsatur violation  51.96\n",
      "num nodes  30  num classes  4  index  10  gnn violation  31.11 cp violation  52.22 greedy violation  52.22 dsatur violation  56.67\n",
      "num nodes  30  num classes  4  index  11  gnn violation  46.15 cp violation  49.45 greedy violation  32.97 dsatur violation  36.26\n",
      "num nodes  30  num classes  4  index  12  gnn violation  47.83 cp violation  31.52 greedy violation  53.26 dsatur violation  46.74\n",
      "num nodes  30  num classes  4  index  13  gnn violation  43.18 cp violation  40.91 greedy violation  56.82 dsatur violation  47.73\n",
      "num nodes  30  num classes  4  index  14  gnn violation  59.34 cp violation  39.56 greedy violation  60.44 dsatur violation  42.86\n",
      "num nodes  30  num classes  4  index  15  gnn violation  63.44 cp violation  53.76 greedy violation  62.37 dsatur violation  52.69\n",
      "num nodes  30  num classes  4  index  16  gnn violation  47.47 cp violation  53.54 greedy violation  51.52 dsatur violation  48.48\n",
      "num nodes  30  num classes  4  index  17  gnn violation  58.33 cp violation  62.5 greedy violation  68.75 dsatur violation  60.42\n",
      "num nodes  30  num classes  4  index  18  gnn violation  46.94 cp violation  47.96 greedy violation  45.92 dsatur violation  47.96\n",
      "num nodes  30  num classes  4  index  19  gnn violation  55.17 cp violation  43.68 greedy violation  55.17 dsatur violation  39.08\n",
      "num nodes  30  num classes  4  index  20  gnn violation  51.32 cp violation  34.21 greedy violation  31.58 dsatur violation  22.37\n",
      "num nodes  30  num classes  4  index  21  gnn violation  57.45 cp violation  40.43 greedy violation  55.32 dsatur violation  52.13\n",
      "num nodes  30  num classes  4  index  22  gnn violation  62.92 cp violation  49.44 greedy violation  64.04 dsatur violation  51.69\n",
      "num nodes  30  num classes  4  index  23  gnn violation  76.92 cp violation  31.87 greedy violation  52.75 dsatur violation  50.55\n",
      "num nodes  30  num classes  4  index  24  gnn violation  54.26 cp violation  61.7 greedy violation  52.13 dsatur violation  48.94\n",
      "num nodes  30  num classes  5  index  0  gnn violation  53.73 cp violation  32.84 greedy violation  37.31 dsatur violation  46.27\n",
      "num nodes  30  num classes  5  index  1  gnn violation  48.85 cp violation  41.22 greedy violation  42.75 dsatur violation  42.75\n",
      "num nodes  30  num classes  5  index  2  gnn violation  49.66 cp violation  32.41 greedy violation  57.93 dsatur violation  40.69\n",
      "num nodes  30  num classes  5  index  3  gnn violation  50.0 cp violation  33.82 greedy violation  44.85 dsatur violation  32.35\n",
      "num nodes  30  num classes  5  index  4  gnn violation  37.01 cp violation  38.58 greedy violation  54.33 dsatur violation  38.58\n",
      "num nodes  30  num classes  5  index  5  gnn violation  38.35 cp violation  54.14 greedy violation  43.61 dsatur violation  44.36\n",
      "num nodes  30  num classes  5  index  6  gnn violation  41.73 cp violation  39.57 greedy violation  51.08 dsatur violation  30.22\n",
      "num nodes  30  num classes  5  index  7  gnn violation  42.22 cp violation  37.04 greedy violation  61.48 dsatur violation  31.85\n",
      "num nodes  30  num classes  5  index  8  gnn violation  49.62 cp violation  30.08 greedy violation  29.32 dsatur violation  30.08\n",
      "num nodes  30  num classes  5  index  9  gnn violation  52.9 cp violation  38.41 greedy violation  63.04 dsatur violation  44.93\n",
      "num nodes  30  num classes  5  index  10  gnn violation  49.67 cp violation  44.37 greedy violation  54.97 dsatur violation  46.36\n",
      "num nodes  30  num classes  5  index  11  gnn violation  56.43 cp violation  39.29 greedy violation  51.43 dsatur violation  34.29\n",
      "num nodes  30  num classes  5  index  12  gnn violation  55.65 cp violation  37.1 greedy violation  45.16 dsatur violation  37.9\n",
      "num nodes  30  num classes  5  index  13  gnn violation  45.65 cp violation  55.07 greedy violation  44.93 dsatur violation  38.41\n",
      "num nodes  30  num classes  5  index  14  gnn violation  46.58 cp violation  25.34 greedy violation  41.1 dsatur violation  44.52\n",
      "num nodes  30  num classes  5  index  15  gnn violation  53.74 cp violation  38.1 greedy violation  59.18 dsatur violation  49.66\n",
      "num nodes  30  num classes  5  index  16  gnn violation  51.11 cp violation  44.44 greedy violation  43.7 dsatur violation  45.93\n",
      "num nodes  30  num classes  5  index  17  gnn violation  39.2 cp violation  48.0 greedy violation  44.8 dsatur violation  49.6\n",
      "num nodes  30  num classes  5  index  18  gnn violation  45.52 cp violation  35.07 greedy violation  38.06 dsatur violation  32.84\n",
      "num nodes  30  num classes  5  index  19  gnn violation  41.73 cp violation  48.2 greedy violation  47.48 dsatur violation  33.81\n",
      "num nodes  30  num classes  5  index  20  gnn violation  48.63 cp violation  38.36 greedy violation  46.58 dsatur violation  34.25\n",
      "num nodes  30  num classes  5  index  21  gnn violation  54.25 cp violation  43.14 greedy violation  59.48 dsatur violation  43.79\n",
      "num nodes  30  num classes  5  index  22  gnn violation  42.19 cp violation  42.19 greedy violation  51.56 dsatur violation  39.06\n",
      "num nodes  30  num classes  5  index  23  gnn violation  46.32 cp violation  46.32 greedy violation  51.47 dsatur violation  44.12\n",
      "num nodes  30  num classes  5  index  24  gnn violation  47.3 cp violation  25.68 greedy violation  60.14 dsatur violation  43.92\n"
     ]
    }
   ],
   "source": [
    "num_nodes_list  = []\n",
    "num_classes_list = []\n",
    "indices = []\n",
    "gnn_violations = []\n",
    "cp_violations = []\n",
    "greedy_violations = []\n",
    "dsatur_violations = []\n",
    "\n",
    "for num_nodes in [30, 40, 50, 60]:\n",
    "    for num_classes in [4, 5]:\n",
    "        for i in range(25):\n",
    "\n",
    "            dataset, raw_data = load_dataset(num_nodes, num_classes, i)\n",
    "\n",
    "            ### Basic information of datasets ###\n",
    "            n = dataset.graph['num_nodes']\n",
    "            e = dataset.graph['edge_index'].shape[1]\n",
    "            # infer the number of classes for non one-hot and one-hot labels\n",
    "            c = num_classes\n",
    "            d = dataset.graph['node_feat'].shape[1]\n",
    "\n",
    "            dataset.graph['edge_index'], dataset.graph['node_feat'] = \\\n",
    "                dataset.graph['edge_index'].to(device), dataset.graph['node_feat'].to(device)\n",
    "\n",
    "\n",
    "            model=NodeFormer(d, hidden_channels, c, num_layers=num_layers, dropout=dropout,\n",
    "                        num_heads=num_heads, use_bn=use_bn, nb_random_features=M,\n",
    "                        use_gumbel=use_gumbel, use_residual=use_residual, use_act=use_act, use_jk=use_jk,\n",
    "                        nb_gumbel_sample=K, rb_order=rb_order, rb_trans=rb_trans).to(device)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            ### Adj storage for relational bias ###\n",
    "            adjs = []\n",
    "            adj, _ = remove_self_loops(dataset.graph['edge_index'])\n",
    "            adj, _ = add_self_loops(adj, num_nodes=n)\n",
    "            adjs.append(adj)\n",
    "            for i in range(0 - 1): # edge_index of high order adjacency\n",
    "                adj = adj_mul(adj, adj, n)\n",
    "                adjs.append(adj)\n",
    "            dataset.graph['adjs'] = adjs\n",
    "\n",
    "            model.reset_parameters()\n",
    "            optimizer = torch.optim.Adam(model.parameters(),weight_decay=5e-3, lr = lr)\n",
    "\n",
    "            losses = []\n",
    "            node_feats = []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out, loss = model(dataset.graph['node_feat'],dataset.graph['edge_index'] ,dataset.graph['adjs'], 0.25)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #print('epoch ', epoch, ' loss ', loss.item())\n",
    "\n",
    "            graph = to_networkx(raw_data, to_undirected= True)\n",
    "            x_final, loss = model(dataset.graph['node_feat'],dataset.graph['edge_index'] ,dataset.graph['adjs'], 0.25)\n",
    "            colors = decoder(x_final)\n",
    "            same_color_edge_count = count_same_color_edges(graph, colors, num_classes)\n",
    "            edges = graph.number_of_edges()\n",
    "            #print(same_color_edge_count, ' out of ', edges)\n",
    "            gnn_violation = 100*same_color_edge_count/edges\n",
    "\n",
    "            ortools_colors = color_graph_with_ortools(graph, num_classes)\n",
    "            same_color_edge_count = count_same_color_edges(graph, ortools_colors, num_classes)\n",
    "            edges = graph.number_of_edges()\n",
    "            #print(same_color_edge_count, ' out of ', edges)\n",
    "            cp_violation = 100*same_color_edge_count/edges\n",
    "\n",
    "            greedy_colors = color_graph(graph, num_classes)\n",
    "            same_color_edge_count = count_same_color_edges(graph, greedy_colors, num_classes)\n",
    "            edges = graph.number_of_edges()\n",
    "            #print(same_color_edge_count, ' out of ', edges)\n",
    "            greedy_violation = 100*same_color_edge_count/edges\n",
    "\n",
    "            dsatur_colors = dsatur(graph)\n",
    "            same_color_edge_count = count_same_color_edges(graph, dsatur_colors, num_classes)\n",
    "            edges = graph.number_of_edges()\n",
    "            #print(same_color_edge_count, ' out of ', edges)\n",
    "            dsatur_violation = 100*same_color_edge_count/edges\n",
    "\n",
    "            num_nodes_list.append(num_nodes)\n",
    "            num_classes_list.append(num_classes)\n",
    "            indices.append(i)\n",
    "            gnn_violations.append(gnn_violation)\n",
    "            cp_violations.append(cp_violation)\n",
    "            greedy_violations.append(greedy_violation)\n",
    "            dsatur_violations.append(dsatur_violation)\n",
    "\n",
    "            print('num nodes ', num_nodes,\n",
    "                ' num classes ', num_classes, \n",
    "                ' index ', i,\n",
    "                ' gnn violation ', round(gnn_violation,2),\n",
    "                'cp violation ', round(cp_violation,2),\n",
    "                'greedy violation ', round(greedy_violation,2),\n",
    "                'dsatur violation ', round(dsatur_violation,2))\n",
    "\n",
    "\n",
    "out_df = pd.DataFrame({'Num_nodes' : num_nodes_list,\n",
    "                       'Num_classes' : num_classes_list,\n",
    "                       'i': indices,\n",
    "                       'GNN' : gnn_violations, \n",
    "                       'CP' : cp_violations,\n",
    "                       'Greedy' : greedy_violations,\n",
    "                       'DSATUR' : dsatur_violations\n",
    "                        })\n",
    "\n",
    "file_path = 'results.csv'\n",
    "\n",
    "# Check if file is empty (or if it exists)\n",
    "if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "    header = False  # Don't write header if file already exists and is not empty\n",
    "else:\n",
    "    header = True  # Write header if file is empty or doesn't exist\n",
    "\n",
    "# Append to CSV\n",
    "out_df.to_csv(file_path, mode='a', header=header, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
